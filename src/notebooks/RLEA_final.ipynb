{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RLEA - final",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1AuBH1GdgNgp3TAaANP33aJ-Hb-29_7QI",
      "authorship_tag": "ABX9TyOa0g8gtNnp1/4uPmBYw0Ib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vuviethung1998/rl-graph-entity-alignment/blob/main/src/notebooks/RLEA_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QLWtFkATkR9"
      },
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwAivE6aTYgT"
      },
      "source": [
        "import gym\n",
        "from scipy import spatial\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from collections import namedtuple, deque\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81FH3GRY3CmJ"
      },
      "source": [
        "def build_adj_matrix_and_embeddings():\n",
        "\n",
        "  # Build adj matrix\n",
        "  file1 = open('/content/drive/MyDrive/Colab Notebooks/RLWithGraphEntityAlignment/data/rel_triples_id_1.txt', 'r')\n",
        "  lines1 = file1.readlines()\n",
        "  lines1 = lines1[:1000]\n",
        "  g1 = []\n",
        "  for line in lines1:\n",
        "    g1.append([int(x) for x in line.strip().split(\"\\t\")])\n",
        "  g1 = np.array(g1)\n",
        "\n",
        "  file2 = open('/content/drive/MyDrive/Colab Notebooks/RLWithGraphEntityAlignment/data/rel_triples_id_2.txt', 'r')\n",
        "  lines2 = file2.readlines()\n",
        "  lines2 = lines2[:1000]\n",
        "  g2 = []\n",
        "  for line in lines2:\n",
        "    g2.append([int(x) for x in line.strip().split(\"\\t\")])\n",
        "  g2 = np.array(g2)\n",
        "  mapping_index_1 = {}\n",
        "  mapping_index_2 = {}\n",
        "  unique1 = np.unique(g1)\n",
        "  unique2 = np.unique(g2)\n",
        "  for i in range(len(unique1)):\n",
        "    mapping_index_1[unique1[i]] = i\n",
        "\n",
        "  for i in range(len(unique2)):\n",
        "    mapping_index_2[unique2[i]] = i\n",
        "\n",
        "  G1_nodes = len(np.unique(g1))\n",
        "  G2_nodes = len(np.unique(g2))\n",
        "  # print(\"num nodes: \", G1_nodes)\n",
        "  # print(\"num nodes: \", G2_nodes)\n",
        "  G1_adj_matrix = np.empty(shape=(G1_nodes, G1_nodes))\n",
        "  G2_adj_matrix = np.empty(shape=(G2_nodes, G2_nodes))\n",
        "  # print(mapping_index_1)\n",
        "  # print(mapping_index_2)\n",
        "  for i in range(len(g1)):\n",
        "    head = g1[i][0]\n",
        "    tail = g1[i][2]\n",
        "    G1_adj_matrix[mapping_index_1[head]][mapping_index_1[tail]] = 1\n",
        "    G1_adj_matrix[mapping_index_1[tail]][mapping_index_1[head]] = 1\n",
        "  for i in range(len(g1)):\n",
        "    head = g2[i][0]\n",
        "    tail = g2[i][2]\n",
        "    G2_adj_matrix[mapping_index_2[head]][mapping_index_2[tail]] = 1\n",
        "    G2_adj_matrix[mapping_index_2[tail]][mapping_index_2[head]] = 1\n",
        "  # print(np.count_nonzero(G1_adj_matrix == 1))\n",
        "  # print(np.count_nonzero(G2_adj_matrix == 1))\n",
        "\n",
        "  # Build embeddings\n",
        "  emb1 = []\n",
        "  emb2 = []\n",
        "  # transitivity = np.load(\"/content/drive/MyDrive/Colab Notebooks/RLWithGraphEntityAlignment/data/transitivity_emb.npy\")\n",
        "  proximi = np.load(\"/content/drive/MyDrive/Colab Notebooks/RLWithGraphEntityAlignment/data/proximi_emb.npy\")\n",
        "  for i, _ in enumerate(mapping_index_1):\n",
        "    emb1.append(proximi[i])\n",
        "  for i, _ in enumerate(mapping_index_2):\n",
        "    emb2.append(proximi[i])\n",
        "  emb1 = np.array(emb1)\n",
        "  emb2 = np.array(emb2)\n",
        "\n",
        "\n",
        "  # Get ground truth\n",
        "  ground_truth = {}\n",
        "  file_gt = open('/content/drive/MyDrive/Colab Notebooks/RLWithGraphEntityAlignment/data/ground_truth.txt', 'r')\n",
        "  lines = file_gt.readlines()\n",
        "  gt = []\n",
        "  for line in lines:\n",
        "    gt.append([int(x) for x in line.strip().split(\"\\t\")])\n",
        "  gt = np.array(gt)\n",
        "  \n",
        "  for i in range(len(gt)):\n",
        "    index_x = gt[i][0]\n",
        "    index_y = gt[i][1]\n",
        "    if index_x in mapping_index_1 and index_y in mapping_index_2:\n",
        "      ground_truth[mapping_index_1[index_x]] = mapping_index_2[index_y] \n",
        "\n",
        "  return G1_adj_matrix, G2_adj_matrix, emb1, emb2, ground_truth"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqwJ9mD5Al_8"
      },
      "source": [
        "G1_adj_matrix, G2_adj_matrix, emb1, emb2, ground_truth = build_adj_matrix_and_embeddings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBJVHZb5n_3A"
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def get_cosine_similarity(vector_1, vector_2):\n",
        "  return spatial.distance.cosine(vector_1, vector_2)\n",
        "\n",
        "def get_k_nearest_items(lst_group_vector_1, lst_group_vector_2, k):\n",
        "  '''\n",
        "    Get 5 vector closest to vector_1\n",
        "    E.g: vector_1 = 3\n",
        "    Return 12345 in lst_group_vector_2 => lay d xong sort \n",
        "  '''\n",
        "  vector = lst_group_vector_1[0]['vector']\n",
        "  sorted_vector_2 = sorted(lst_group_vector_2, key=(lambda t: get_cosine_similarity(t['vector'], vector)) )\n",
        "  res = sorted_vector_2[0:k]\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du502CRfesfA"
      },
      "source": [
        "# Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULKqLdLwesBx"
      },
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'reward'))\n",
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.memory = deque()\n",
        "\n",
        "    def push(self, state, action, reward):\n",
        "        self.memory.append(Transition(state, action, reward))\n",
        "\n",
        "    def sample(self):\n",
        "        memory = self.memory\n",
        "        return Transition(*zip(*memory)) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCdkyY8ElfIn"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8-KWKyY18EB"
      },
      "source": [
        "class ValueFunctionNet(nn.Module):\n",
        "    def __init__(self, state_size, hidden_size):\n",
        "        super(ValueFunctionNet, self).__init__()\n",
        "        self.dense_layer = nn.Linear(state_size, hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = nn.Sigmoid(self.dense_layer(x))\n",
        "        return self.output(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ8U0496KH6n"
      },
      "source": [
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "class GCN_layer(nn.Module):\n",
        "    \"\"\"\n",
        "      Define filter layer 1/2 like in the above image\n",
        "      Calculate A_hat first then,\n",
        "      Input: adj_matrix with input features X\n",
        "    \"\"\"   \n",
        "\n",
        "    def __init__(self, first_adj_matrix, second_adj_matrix, inputs_shape, outputs_shape):\n",
        "        super(GCN_layer, self).__init__()\n",
        "\n",
        "        self.W=Parameter(torch.rand(inputs_shape,outputs_shape),requires_grad=True)\n",
        "        self.bias = Parameter(torch.rand(outputs_shape),requires_grad=True)\n",
        "        A=torch.from_numpy(first_adj_matrix).type(torch.LongTensor).to(device)\n",
        "        I=torch.eye(A.shape[0]).to(device)\n",
        "        A_hat=A+I\n",
        "        D=torch.sum(A_hat,axis=0)\n",
        "        D=torch.diag(D)\n",
        "        D_inv=torch.inverse(D)\n",
        "        self.A_hat_x = torch.mm(torch.mm(D_inv,A_hat),D_inv)\n",
        "        \n",
        "        A=torch.from_numpy(second_adj_matrix).type(torch.LongTensor).to(device).to(device)\n",
        "        I=torch.eye(A.shape[0]).to(device)\n",
        "        A_hat=A+I\n",
        "        D=torch.sum(A_hat,axis=0)\n",
        "        D=torch.diag(D)\n",
        "        D_inv=torch.inverse(D)\n",
        "        self.A_hat_y = torch.mm(torch.mm(D_inv,A_hat),D_inv)\n",
        "    \n",
        "    def forward(self, i, input_features):\n",
        "        if i == \"x\":\n",
        "          aggregate=torch.mm(self.A_hat_x, input_features)\n",
        "        else: \n",
        "          aggregate=torch.mm(self.A_hat_y, input_features)    \n",
        "        propagate=torch.mm(aggregate, self.W)+self.bias             \n",
        "        return propagate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lARwv5ByBe1"
      },
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self, first_adj_matrix, second_adj_matrix, inputs_shape,outputs_shape,n_classes,activation='Sigmoid'):\n",
        "        super(Agent, self).__init__()\n",
        "\n",
        "        self.layer1=GCN_layer(first_adj_matrix, second_adj_matrix, inputs_shape, outputs_shape)\n",
        "        self.layer2=GCN_layer(first_adj_matrix, second_adj_matrix, outputs_shape, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        if activation =='Tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation=='Sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation=='Softmax':\n",
        "            self.activation=nn.Softmax()\n",
        "        elif activation=='Relu':\n",
        "            self.activation=nn.ReLU()\n",
        "    \n",
        "        self.softmax=nn.Softmax(dim=1)\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "        self.W_h=Parameter(torch.rand(1, 2), requires_grad=True)\n",
        "        self.W_f=Parameter(torch.rand(n_classes, n_classes), requires_grad=True)\n",
        "        self.W_p=Parameter(torch.rand(1, 1), requires_grad=True)\n",
        "        self.bias_h = Parameter(torch.rand(1),requires_grad=True)\n",
        "    \n",
        "    # Gett k nearest opponents for mutual information estimator\n",
        "    def get_k_nearest_opponent(self, G, node, k=3):\n",
        "        G_list = [(i, item) for i, item in enumerate(G)]\n",
        "        # Tìm nearest thế này là gồm cả chính node đó\n",
        "        nearest_node = sorted(G_list, key=(lambda other_node: F.cosine_similarity(torch.reshape(other_node[1], (1, -1)), node)), reverse=True)\n",
        "        k_nearest_opponent = nearest_node[:k]\n",
        "        k_nearest_opponent_vector = [G[item[0]] for item in k_nearest_opponent]\n",
        "        return k_nearest_opponent_vector\n",
        "\n",
        "    def forward(self, first_embeddings, second_embeddings, state):\n",
        "        index_x = state[0]\n",
        "        index_y = state[1]\n",
        "        x=self.layer1(\"x\", first_embeddings)\n",
        "        x=self.activation(x)\n",
        "        x=self.layer2(\"x\", x)\n",
        "        G_x = self.activation(x)\n",
        "\n",
        "        y=self.layer1(\"y\", second_embeddings)\n",
        "        y=self.activation(y) \n",
        "        y=self.layer2(\"y\", y)\n",
        "        G_y = self.activation(y)\n",
        "        g_x = torch.reshape(G_x[index_x], (1, self.n_classes))\n",
        "        g_y = torch.reshape(G_y[index_y], (1, self.n_classes))\n",
        "\n",
        "        # Linear combination\n",
        "        cat_gxgy = torch.cat((g_x, g_y), 0)\n",
        "        h = self.sigmoid(torch.mm(self.W_h, cat_gxgy) + self.bias_h)\n",
        "\n",
        "        # Mutual information estimator\n",
        "        f = torch.exp(g_x.T*self.W_f*g_y)\n",
        "        k_nearest_opponent_vector = self.get_k_nearest_opponent(G_y, g_y, k=11) # Bao gồm cả chính node e_y nên lấy k=11. Paper nói là k=10\n",
        "        list_temp = [torch.exp(g_x.T*self.W_f*oppo) for oppo in k_nearest_opponent_vector]\n",
        "        f_oppo = torch.stack(list_temp).sum()\n",
        "        I = f/f_oppo\n",
        "\n",
        "        # Policy\n",
        "        policy = self.softmax(torch.mm(self.W_p, torch.cat((h, I), 1)))\n",
        "        return policy\n",
        "\n",
        "    # Train model using reinforcement learning\n",
        "    @classmethod\n",
        "    def train_model(self, first_embeddings, second_embeddings, net, transitions, optimizer):\n",
        "        states, actions, rewards = transitions.state, transitions.action, transitions.reward\n",
        "\n",
        "        actions = torch.stack(actions).to(device)\n",
        "        rewards = torch.Tensor(rewards).to(device)\n",
        "        returns = torch.zeros_like(rewards).to(device)\n",
        "        vf = torch.zeros_like(rewards).to(device)\n",
        "        total_loss = 0\n",
        "        running_return = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            # calculate G from the last transition\n",
        "            running_return = gamma**0 * rewards[t] + gamma * running_return\n",
        "            returns[t] = running_return\n",
        "            if returns.sum() == 0:\n",
        "              vf[t] = 0.01\n",
        "            else:\n",
        "              vf[t] = running_return/returns.sum()\n",
        "            # get value function estimates\n",
        "            advantage = returns[t] - vf[t]\n",
        "\n",
        "            # loss\n",
        "            policies = net(first_embeddings, second_embeddings, states[t])\n",
        "            # sum all features/embedding vectors of the state\n",
        "            log_policies = (torch.log(policies) * actions[t].detach()).sum(dim=1)\n",
        "            loss = (-log_policies * advantage).sum()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss\n",
        "        \n",
        "        return total_loss\n",
        "\n",
        "    def get_action(self, first_embeddings, second_embeddings, state):\n",
        "        policy = self.forward(first_embeddings, second_embeddings, state)\n",
        "        p = policy[0].cpu().data.numpy()\n",
        "        action = np.random.choice(2, 1, p=p)[0]\n",
        "        return action"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlkAynpIkrsP"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQtXa6W_cfY0"
      },
      "source": [
        "G1_adj_matrix, G2_adj_matrix, emb1, emb2, ground_truth = build_adj_matrix_and_embeddings()\n",
        "from scipy import spatial\n",
        "def get_k_nearest_candidate(target_node, k=11):\n",
        "  nearest_list = [(i, emb2[i]) for i in range(len(emb2))]\n",
        "  list_similarity = sorted(nearest_list, key=(lambda nearest_node: 1 - spatial.distance.cosine(nearest_node[1], emb2[target_node])), reverse=True)\n",
        "  list_similarity = list_similarity[:k]\n",
        "  k_nearest_nodes = [item[0] for item in list_similarity]\n",
        "  return k_nearest_nodes\n",
        "\n",
        "def getHashTable2():\n",
        "  hash_mapping_node = {}\n",
        "  for source_node, target_node in ground_truth.items():\n",
        "    hash_mapping_node[source_node] = get_k_nearest_candidate(target_node)\n",
        "  return hash_mapping_node"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2toDi7coEXg"
      },
      "source": [
        "# đổi lại thành dạng hash table \n",
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import spatial\n",
        "\n",
        "G1_adj_matrix, G2_adj_matrix, emb1, emb2, ground_truth = build_adj_matrix_and_embeddings()\n",
        "\n",
        "def get_k_nearest_candidate(target_node, emb2, k=11):\n",
        "  nearest_list = [(i, emb2[i]) for i in range(len(emb2))]\n",
        "  list_similarity = sorted(nearest_list, key=(lambda nearest_node: 1 - spatial.distance.cosine(nearest_node[1], emb2[target_node])), reverse=True)\n",
        "  list_similarity = list_similarity[:k]\n",
        "  k_nearest_nodes = [item[0] for item in list_similarity]\n",
        "  return k_nearest_nodes\n",
        "\n",
        "def getHashTable(ground_truth, emb2):\n",
        "  '''\n",
        "    Input: groundtruth, \n",
        "  '''\n",
        "  hash_mapping_node = {}\n",
        "  for source_node, target_node in ground_truth.items():\n",
        "    hash_mapping_node[source_node] = get_k_nearest_candidate(target_node, emb2)\n",
        "  return hash_mapping_node\n",
        "\n",
        "def getAlignTable():\n",
        "  return ground_truth\n",
        "\n",
        "def getListState(hash_table):\n",
        "  lst_state = []\n",
        "  for key in hash_table.keys():\n",
        "    lst_key_2 = hash_table[key]\n",
        "    for key_2 in lst_key_2:\n",
        "      lst_state.append((int(key), key_2))\n",
        "  return lst_state\n",
        "\n",
        "def getNodeEmbeddingByKey(emb, id):\n",
        "  return emb[id]\n",
        "\n",
        "def popCurrentState(lst_state, current_state):\n",
        "  # print('currstate: ', current_state)\n",
        "  key1, key2 = current_state[0], current_state[1]\n",
        "  # print((key1, key2))\n",
        "\n",
        "  lst_remove = []\n",
        "  for state in lst_state:\n",
        "    key_state_1 = state[0]\n",
        "    key_state_2 = state[1]\n",
        "    # print('cur_state: ', state)\n",
        "\n",
        "    if key_state_1==key1 or key_state_2 == key1 or key_state_1==key2 or key_state_2==key2:\n",
        "      lst_remove.append(state)\n",
        "  lst_final = list(set(lst_state) - set(lst_remove))\n",
        "  return lst_final\n",
        "    \n",
        "def isAligned(state):\n",
        "  alignTable = getAlignTable()\n",
        "  true_state = (state[0], alignTable[state[0]])\n",
        "  if state==true_state:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "class SequentialMatchingEnv(gym.Env): \n",
        "  \"\"\"\n",
        "  Custom Environment for Binary Scheme\n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for action\n",
        "  MATCH = 1\n",
        "  UNMATCH = 0  \n",
        "  \n",
        "  MIN_SKIP_RATE = 0.01\n",
        "  BASIC_SKIP_RATE = 0.8\n",
        "  DISCOUNT_RATIO = 0.9\n",
        "  THETA = 1\n",
        "\n",
        "  def __init__(self, k_nearest=5 ):\n",
        "    super(SequentialMatchingEnv, self).__init__()\n",
        "\n",
        "    self.seed = random.randint(0,100)\n",
        "\n",
        "    # init no vectors\n",
        "    # self.vector_size = vector_size\n",
        "    # self.k_nearest = k_nearest\n",
        "    \n",
        "    self.hash_table = getHashTable(ground_truth, emb2)\n",
        "    self.list_state = getListState(self.hash_table)\n",
        "    # init total reward   \n",
        "    self.total_reward = 0 \n",
        "\n",
        "    # define action and observation space \n",
        "    n_actions = 2\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "\n",
        "    # observation will be the source vector and the target vector with each (1,5)vector \n",
        "    self.observation_space =  spaces.Box(low=0, high=1, shape=(2,5), dtype=np.float16)\n",
        "    \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "      Important: The observation must be numpy array \n",
        "      : return: (np.array)\n",
        "    \"\"\"\n",
        "    self.lst_state = getListState(self.hash_table) # get all state possible \n",
        "\n",
        "    first_state = self.lst_state[0]\n",
        "    self.state_embedding = np.array([getNodeEmbeddingByKey(emb1, first_state[0]), \n",
        "                               getNodeEmbeddingByKey(emb2, first_state[1])])\n",
        "    return self.lst_state[0]\n",
        "\n",
        "  def step(self, action, episode_num=1):\n",
        "\n",
        "    self.episode_num = episode_num\n",
        "    current_state = self.lst_state[0]\n",
        "\n",
        "    # cur_cosine_sim = get_cosine_similarity(getNodeEmbeddingByKey(1, current_state[0]), getNodeEmbeddingByKey(2, current_state[1]))\n",
        "    if action == self.MATCH:\n",
        "      # if match pop all state containing two key of current state \n",
        "      self.lst_state = popCurrentState(self.lst_state, current_state)\n",
        "\n",
        "      # check value if true match\n",
        "      if isAligned(current_state): # label = 1 => difficulty = Cemax,ex - Cexey\n",
        "        # difficulty = max_cosine_sim - cur_cosine_sim \n",
        "        # skip_rate = max(self.MIN_SKIP_RATE, (self.DISCOUNT_RATIO ** (self.episode_num - 1)) * self.BASIC_SKIP_RATE * difficulty)\n",
        "\n",
        "        # random_p = random.random()\n",
        "        # if random_p < skip_rate:\n",
        "        #   score = 0\n",
        "        # else:\n",
        "        score = 1\n",
        "\n",
        "      elif not isAligned(current_state): # label = 0 => difficulty = theta - (Cemax,ex - Cexey)\n",
        "        # difficulty = self.THETA - (max_cosine_sim - cur_cosine_sim)\n",
        "        # skip_rate = max(self.MIN_SKIP_RATE, (self.DISCOUNT_RATIO ** (self.episode_num - 1)) * self.BASIC_SKIP_RATE * difficulty)\n",
        "        # random_p = random.random()\n",
        "        # if random_p > skip_rate: # neu k skip\n",
        "        score = 0\n",
        "        # else: score = 0 # neu skip, score = 0 va \n",
        "    elif action == self.UNMATCH:\n",
        "      # if mismatch pop head of group vector 2 \n",
        "      self.lst_state.pop(0)\n",
        "\n",
        "      # if true mismatch -> LABEL = 0 => difficulty = theta - (Cemax,ex - Cexey)\n",
        "      if not isAligned(current_state): \n",
        "        # difficulty = self.THETA - (max_cosine_sim - cur_cosine_sim)\n",
        "        # skip_rate = max(self.MIN_SKIP_RATE, (self.DISCOUNT_RATIO ** (self.episode_num - 1)) * self.BASIC_SKIP_RATE * difficulty)\n",
        "        # skip hay khong deu khong anh huong\n",
        "        score = 0 \n",
        "      # if false mismatch \n",
        "      elif isAligned(current_state): \n",
        "        # difficulty = max_cosine_sim - cur_cosine_sim\n",
        "        # skip_rate = max(self.MIN_SKIP_RATE, (self.DISCOUNT_RATIO ** (self.episode_num - 1)) * self.BASIC_SKIP_RATE * difficulty)\n",
        "\n",
        "        # random_p = random.random()\n",
        "        # if random_p < skip_rate:\n",
        "        #   score = 0\n",
        "        # else:\n",
        "        score = -10\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "    info = {}\n",
        "\n",
        "    if len(self.lst_state) == 0:\n",
        "      next_embedding = None\n",
        "      next_state = (None, None) # khong con phan tu nao trong G1\n",
        "      done = True\n",
        "    else:\n",
        "      # get next state id and next state embedding \n",
        "      next_state = self.lst_state[0]\n",
        "      next_embedding = np.array([getNodeEmbeddingByKey(emb1, next_state[0]), \n",
        "                               getNodeEmbeddingByKey(emb2, next_state[1])])\n",
        "      done = False\n",
        "    return current_state, next_state, next_embedding, score, done, info"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6hW51pLhQ-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bbfe67-12e0-400a-9767-f404fbdff25b"
      },
      "source": [
        "hash_mapping_node = getHashTable(ground_truth, emb2)\n",
        "print(hash_mapping_node)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{196: [188, 93, 742, 711, 718, 337, 138, 227, 288, 77, 70], 1359: [1461, 999, 7, 128, 34, 280, 407, 390, 1648, 1107, 1650], 191: [154, 367, 1619, 1018, 1321, 1264, 989, 1291, 1543, 450, 1292], 689: [647, 810, 749, 748, 463, 538, 911, 556, 431, 670, 491], 402: [419, 454, 703, 658, 228, 247, 421, 1329, 341, 1115, 1302], 144: [189, 170, 436, 447, 865, 543, 113, 958, 24, 660, 987], 96: [71, 94, 1574, 1343, 212, 459, 752, 555, 165, 554, 1449], 47: [38, 109, 130, 257, 210, 233, 84, 688, 472, 357, 522], 1597: [1162, 1087, 1488, 1355, 1658, 1145, 541, 490, 123, 267, 355], 114: [105, 168, 253, 296, 598, 47, 580, 405, 879, 992, 1217], 52: [68, 92, 282, 207, 738, 662, 215, 101, 387, 230, 1225], 382: [330, 275, 574, 206, 1038, 57, 384, 453, 923, 58, 155], 159: [136, 139, 1163, 498, 245, 626, 621, 125, 278, 144, 135], 668: [1098, 1603, 720, 1467, 466, 1476, 1472, 962, 140, 995, 590], 82: [59, 62, 1267, 1224, 1353, 347, 1410, 182, 223, 603, 624], 334: [319, 272, 1227, 1522, 891, 1524, 252, 712, 778, 253, 14], 43: [39, 64, 98, 67, 934, 582, 557, 1374, 793, 789, 1142], 19: [51, 32, 417, 322, 368, 433, 392, 785, 477, 428, 712], 321: [210, 233, 38, 109, 688, 84, 411, 1256, 357, 130, 715], 1: [16, 17, 26, 85, 1421, 18, 1188, 23, 1260, 147, 1191], 502: [535, 880, 1352, 1379, 1324, 847, 1450, 1445, 751, 911, 733], 182: [165, 350, 388, 355, 332, 297, 554, 250, 294, 290, 895], 78: [92, 101, 68, 215, 449, 482, 230, 282, 207, 773, 1580], 827: [1031, 1124, 1221, 833, 1054, 693, 1405, 1602, 373, 176, 1686], 76: [61, 118, 281, 314, 206, 330, 574, 275, 57, 311, 973], 95: [85, 26, 16, 17, 1421, 1188, 1032, 1625, 1642, 1515, 142], 507: [374, 425, 1230, 1095, 1464, 1253, 1077, 1390, 175, 407, 218], 343: [265, 376, 250, 241, 1540, 474, 829, 794, 852, 633, 133], 21: [14, 15, 135, 252, 144, 197, 102, 308, 229, 120, 424], 662: [742, 711, 1187, 375, 1474, 188, 93, 1391, 276, 70, 701], 197: [215, 449, 482, 101, 92, 230, 68, 207, 79, 282, 503], 24: [35, 70, 287, 134, 593, 1112, 1330, 312, 270, 185, 603], 392: [346, 309, 465, 1015, 1559, 1131, 55, 192, 1546, 51, 524], 856: [529, 446, 1504, 1403, 191, 417, 922, 1044, 969, 128, 663], 275: [259, 192, 28, 9, 89, 1555, 736, 1387, 143, 973, 574], 975: [561, 650, 684, 541, 727, 685, 229, 842, 502, 359, 490], 461: [452, 505, 1309, 893, 913, 438, 340, 881, 818, 164, 745], 213: [223, 182, 70, 603, 523, 35, 593, 287, 569, 1439, 851], 673: [752, 555, 554, 877, 755, 1187, 1082, 862, 701, 1474, 670], 508: [579, 672, 124, 239, 869, 826, 1158, 145, 140, 209, 981], 712: [987, 1022, 207, 705, 189, 282, 1430, 543, 643, 1372, 1097], 1045: [915, 1216, 862, 701, 337, 1342, 99, 911, 176, 1648, 128], 85: [94, 71, 1574, 1343, 212, 459, 165, 350, 752, 554, 555], 266: [220, 269, 572, 509, 788, 429, 37, 48, 1063, 40, 290], 887: [1187, 1474, 375, 742, 755, 711, 1082, 430, 752, 862, 555], 33: [22, 13, 80, 77, 27, 435, 393, 302, 1640, 52, 553], 80: [46, 31, 591, 340, 398, 532, 405, 345, 645, 754, 359], 112: [104, 63, 1055, 1689, 1020, 76, 1454, 466, 1062, 998, 850], 13: [40, 37, 21, 269, 12, 48, 220, 32, 841, 678, 217], 608: [539, 400, 1633, 1511, 1336, 1170, 459, 75, 212, 455, 157], 826: [842, 685, 315, 290, 262, 596, 416, 509, 788, 826, 571], 137: [128, 191, 1210, 773, 262, 251, 51, 32, 322, 1480, 1075], 961: [1030, 781, 378, 385, 74, 1367, 625, 1418, 705, 956, 1194], 733: [797, 802, 1363, 1138, 1526, 1581, 1685, 1394, 1383, 1507, 1335], 482: [477, 428, 1183, 964, 1259, 487, 638, 546, 523, 51, 894], 459: [365, 284, 982, 1102, 427, 616, 1534, 467, 661, 1370, 986], 40: [36, 53, 143, 106, 1606, 520, 1344, 409, 946, 322, 417], 57: [24, 113, 1021, 1541, 170, 436, 189, 447, 10, 301, 204], 573: [606, 2, 33, 1454, 1276, 1358, 1598, 1600, 1204, 1042, 781], 434: [388, 355, 350, 165, 178, 205, 1123, 1444, 554, 1488, 18], 124: [125, 278, 621, 626, 245, 139, 136, 498, 1163, 1421, 1188], 784: [807, 856, 777, 926, 613, 325, 1320, 814, 43, 44, 441], 400: [449, 482, 215, 101, 92, 230, 207, 68, 356, 1032, 1498], 549: [431, 578, 116, 677, 55, 538, 185, 70, 777, 749, 503], 646: [616, 427, 1534, 1102, 982, 284, 365, 1130, 467, 636, 27], 375: [363, 231, 360, 723, 892, 280, 144, 135, 1286, 491, 321], 142: [209, 124, 239, 738, 387, 503, 79, 1032, 479, 50, 282], 552: [499, 494, 72, 66, 45, 632, 1191, 123, 633, 290, 1298], 740: [936, 761, 422, 371, 523, 546, 1385, 425, 374, 8, 563], 394: [470, 335, 1374, 789, 1409, 1376, 1526, 138, 793, 1120, 1085], 59: [69, 10, 1471, 4, 909, 1278, 861, 11, 197, 252, 722], 663: [809, 472, 522, 357, 84, 1196, 109, 715, 233, 257, 130], 425: [341, 410, 228, 247, 73, 393, 156, 414, 128, 585, 48], 249: [262, 251, 191, 128, 773, 842, 826, 1543, 214, 645, 1075], 308: [327, 608, 1003, 516, 1035, 644, 70, 131, 1442, 423, 370], 81: [88, 95, 458, 471, 1488, 198, 1355, 159, 1085, 1083, 1154], 113: [86, 83, 173, 122, 1509, 1670, 23, 302, 1340, 1668, 1662], 419: [511, 640, 767, 502, 359, 345, 1130, 532, 1075, 518, 1406], 22: [21, 12, 37, 151, 40, 126, 674, 436, 60, 119, 368], 749: [951, 1286, 545, 280, 806, 231, 775, 896, 852, 360, 363], 1410: [1357, 1094, 1195, 827, 734, 595, 1233, 1262, 899, 974, 1271], 143: [122, 173, 86, 83, 383, 933, 1010, 408, 806, 469, 312], 1394: [1377, 1436, 619, 920, 969, 1044, 1426, 1635, 283, 1451, 1658], 309: [312, 469, 70, 593, 1112, 35, 1330, 270, 287, 134, 185], 421: [393, 414, 271, 302, 13, 228, 247, 410, 341, 32, 191], 44: [45, 66, 292, 1664, 494, 1269, 351, 405, 649, 499, 584], 26: [13, 22, 393, 414, 302, 271, 464, 30, 80, 77, 5], 29: [28, 9, 259, 132, 81, 89, 192, 96, 655, 487, 638], 1177: [1105, 1594, 1418, 1443, 1354, 1293, 681, 512, 385, 378, 939], 974: [786, 485, 217, 565, 1688, 1259, 721, 766, 777, 1176, 1209], 153: [79, 50, 209, 207, 683, 479, 622, 1202, 503, 282, 148], 753: [880, 535, 1379, 1352, 1324, 847, 911, 553, 1445, 1450, 346], 851: [562, 439, 1100, 1663, 1121, 1228, 1166, 1525, 185, 870, 233], 357: [299, 342, 320, 331, 128, 402, 940, 1562, 833, 1418, 1210], 15: [2, 33, 606, 1454, 74, 625, 1276, 1603, 590, 540, 1128], 408: [585, 678, 279, 274, 341, 972, 410, 1015, 156, 128, 1632], 1241: [857, 990, 1669, 754, 1483, 929, 436, 351, 886, 447, 292], 1219: [1339, 1650, 1648, 1082, 947, 843, 337, 1570, 430, 755, 55], 1018: [745, 1308, 164, 1079, 893, 438, 1466, 913, 1692, 339, 1413], 287: [183, 294, 1140, 771, 1364, 801, 776, 719, 872, 530, 946], 541: [324, 919, 1176, 1209, 1164, 747, 1424, 721, 889, 485, 874], 585: [646, 673, 193, 1497, 794, 1490, 816, 196, 633, 609, 1008], 108: [180, 127, 214, 391, 1572, 199, 1206, 600, 1001, 222, 1075], 110: [101, 92, 215, 449, 482, 230, 68, 207, 282, 1689, 503], 162: [144, 135, 723, 892, 360, 363, 252, 15, 14, 621, 197], 1075: [943, 1202, 869, 149, 662, 503, 1070, 533, 108, 387, 479], 1091: [1254, 1423, 1, 1598, 1090, 1314, 777, 1347, 855, 1589, 570], 262: [232, 211, 871, 206, 698, 643, 580, 922, 663, 1076, 569], 1677: [1577, 1478, 1064, 1392, 668, 1406, 1202, 1360, 943, 941, 743], 158: [143, 106, 53, 36, 875, 1026, 1606, 259, 961, 651, 192], 151: [156, 137, 438, 893, 341, 370, 166, 644, 306, 131, 303], 356: [356, 317, 1202, 1402, 831, 267, 5, 158, 30, 943, 839], 111: [84, 357, 522, 1196, 472, 715, 109, 233, 809, 1578, 38], 299: [260, 201, 354, 855, 353, 570, 1569, 1506, 294, 894, 447], 349: [424, 229, 530, 771, 1140, 1287, 614, 51, 14, 82, 1085], 41: [12, 21, 151, 40, 37, 126, 1118, 60, 119, 902, 1246], 437: [417, 322, 51, 1302, 1329, 32, 119, 368, 126, 410, 1530], 1080: [1348, 1282, 1323, 1493, 991, 890, 698, 1185, 1387, 1143, 1234], 1370: [1364, 1694, 183, 504, 294, 811, 489, 1276, 859, 1219, 42], 1314: [1224, 1267, 59, 62, 495, 246, 29, 1000, 1327, 1181, 539], 20: [18, 23, 290, 17, 877, 554, 16, 315, 388, 355, 467], 462: [475, 514, 1486, 1351, 1282, 94, 323, 694, 1405, 1649, 1062], 104: [91, 138, 783, 706, 358, 224, 147, 285, 197, 93, 1140], 150: [242, 115, 1279, 884, 1039, 1168, 1089, 445, 1537, 418, 655], 981: [864, 1081, 1296, 1119, 714, 582, 1152, 966, 53, 1341, 608], 458: [447, 436, 189, 1501, 170, 354, 1372, 353, 113, 24, 857], 353: [282, 207, 230, 68, 149, 662, 108, 92, 124, 101, 209], 1400: [1301, 76, 243, 226, 65, 1597, 476, 1527, 389, 174, 113], 289: [333, 304, 634, 689, 851, 456, 630, 1073, 99, 176, 558], 11: [6, 87, 1370, 1063, 1134, 1405, 1602, 693, 1534, 1659, 1424], 499: [569, 894, 871, 1076, 1140, 477, 428, 571, 674, 232, 8], 6: [10, 69, 1471, 4, 909, 1278, 11, 861, 252, 197, 722], 570: [571, 596, 1463, 416, 1378, 651, 740, 263, 315, 42, 286], 559: [502, 359, 3, 640, 511, 532, 345, 405, 248, 398, 1118], 756: [961, 1236, 1026, 875, 1346, 143, 740, 732, 651, 433, 1247], 1462: [1448, 229, 1289, 778, 424, 1516, 524, 659, 853, 1371, 1624], 609: [479, 387, 662, 503, 738, 869, 209, 79, 149, 50, 124], 540: [463, 670, 483, 300, 337, 780, 430, 629, 752, 647, 555], 231: [200, 181, 403, 386, 832, 236, 1050, 1297, 1215, 885, 235], 1046: [703, 658, 419, 454, 421, 992, 1115, 1217, 247, 228, 563], 558: [460, 293, 297, 294, 696, 894, 332, 871, 1673, 133, 5], 161: [124, 209, 738, 239, 387, 1070, 503, 869, 662, 240, 579], 374: [162, 117, 531, 746, 653, 1226, 1036, 1273, 305, 1109, 1514], 279: [432, 581, 1659, 1550, 717, 942, 141, 894, 1278, 1168, 1065], 563: [559, 612, 1502, 1456, 1139, 718, 1195, 1455, 1094, 1086, 250], 526: [386, 403, 1074, 1215, 832, 181, 200, 236, 885, 1297, 235], 84: [50, 79, 622, 683, 503, 1202, 209, 207, 479, 325, 148], 127: [106, 143, 53, 36, 1026, 875, 1606, 740, 15, 259, 736], 882: [630, 1073, 333, 304, 1321, 99, 1018, 176, 863, 1122, 1473], 25: [37, 40, 21, 269, 48, 220, 32, 12, 1691, 73, 415], 87: [56, 1475, 1298, 1638, 492, 1191, 1327, 1551, 1453, 736, 645], 748: [947, 1648, 1339, 1650, 1082, 755, 1340, 463, 718, 186, 1302], 450: [505, 452, 893, 1309, 438, 1692, 913, 1088, 726, 745, 1413], 362: [305, 254, 162, 117, 7, 438, 1333, 34, 1143, 1412, 1234], 79: [87, 6, 1063, 1370, 1405, 1134, 1602, 1124, 833, 693, 1659], 269: [176, 99, 476, 304, 333, 1220, 389, 822, 915, 898, 630], 252: [120, 49, 135, 144, 15, 592, 465, 14, 1082, 1691, 1650], 409: [766, 565, 874, 721, 506, 786, 25, 453, 632, 1543, 118], 354: [484, 637, 198, 0, 496, 491, 159, 1, 7, 34, 999], 791: [990, 857, 1669, 754, 1483, 1642, 1388, 1255, 714, 170, 436], 1415: [1642, 1255, 828, 791, 1370, 990, 1669, 1366, 754, 1063, 26], 204: [170, 189, 436, 660, 447, 958, 543, 113, 24, 865, 1182], 48: [32, 51, 126, 322, 271, 712, 392, 785, 191, 302, 417], 117: [112, 41, 1200, 935, 1058, 765, 756, 418, 589, 474, 445], 581: [252, 197, 144, 14, 135, 15, 1001, 1206, 1265, 336, 491], 1337: [1238, 1213, 1616, 1415, 1016, 1059, 734, 1011, 827, 823, 1280], 911: [909, 1278, 1362, 1046, 989, 1554, 1628, 69, 10, 1471, 652], 16: [25, 82, 54, 817, 1335, 1001, 1206, 161, 766, 1240, 986], 1125: [683, 622, 50, 79, 503, 1202, 695, 207, 882, 230, 1580], 782: [885, 832, 1215, 386, 245, 235, 403, 236, 181, 1510, 200], 53: [117, 162, 531, 746, 653, 1226, 1036, 1273, 1109, 1514, 305], 1246: [1575, 43, 556, 814, 1597, 1218, 459, 212, 44, 1580, 1256], 55: [27, 52, 467, 636, 332, 297, 325, 1422, 616, 1062, 816], 83: [97, 90, 123, 303, 1353, 306, 35, 553, 464, 93, 72], 1610: [1472, 1467, 590, 1128, 700, 1470, 33, 1048, 1042, 1559, 1194], 1202: [923, 1038, 147, 330, 224, 57, 275, 824, 1549, 58, 206], 31: [29, 42, 416, 965, 65, 651, 1267, 941, 1224, 528, 596], 12: [53, 36, 143, 106, 1606, 1026, 1344, 875, 409, 520, 946], 1523: [1516, 1625, 1418, 1293, 378, 385, 1387, 214, 163, 736, 318], 596: [736, 1387, 890, 991, 1323, 163, 192, 213, 318, 518, 698], 811: [1109, 1514, 117, 955, 162, 1226, 1036, 1668, 1471, 653, 799], 1283: [1303, 942, 654, 623, 508, 534, 772, 1019, 694, 560, 1691], 69: [60, 119, 126, 322, 414, 228, 151, 1378, 458, 1463, 417], 190: [167, 208, 861, 1521, 307, 1488, 1355, 308, 183, 801, 722], 72: [64, 39, 98, 934, 67, 582, 557, 1374, 750, 698, 1142], 180: [146, 187, 373, 1115, 1192, 970, 489, 588, 1518, 700, 1118], 17: [3, 8, 398, 359, 502, 894, 569, 144, 892, 523, 546], 342: [390, 407, 54, 76, 368, 1329, 51, 421, 237, 352, 405], 420: [455, 1122, 765, 935, 157, 1058, 216, 753, 1200, 379, 527], 463: [508, 1404, 244, 519, 660, 543, 1303, 1550, 1676, 654, 942], 14: [4, 11, 1598, 10, 69, 1175, 1000, 1198, 401, 1169, 970], 243: [292, 351, 584, 649, 45, 66, 1376, 714, 1372, 576, 1501], 58: [90, 97, 35, 267, 464, 188, 123, 553, 303, 93, 13], 172: [164, 1079, 745, 1308, 686, 1011, 141, 777, 901, 96, 478], 528: [567, 576, 1569, 1506, 1056, 1085, 29, 315, 1173, 290, 176], 993: [1602, 1405, 413, 6, 87, 1124, 1271, 1031, 1288, 1221, 1508], 825: [838, 1045, 291, 110, 516, 837, 868, 696, 679, 382, 1074], 436: [1189, 904, 191, 823, 128, 944, 1660, 1316, 1494, 1210, 1090], 557: [345, 532, 405, 7, 34, 591, 340, 502, 359, 1052, 684], 632: [890, 991, 1323, 736, 1387, 698, 1185, 163, 318, 518, 1647], 177: [197, 252, 144, 135, 14, 15, 1001, 1206, 1265, 91, 308], 597: [574, 973, 330, 118, 314, 275, 281, 259, 61, 428, 206], 200: [412, 361, 788, 132, 89, 509, 1300, 1587, 1490, 855, 1398], 884: [494, 499, 66, 45, 290, 72, 632, 794, 633, 1191, 580], 1180: [1382, 709, 404, 645, 492, 128, 191, 1014, 1196, 769, 603], 51: [66, 45, 1664, 494, 1269, 499, 292, 405, 351, 649, 290], 285: [182, 223, 70, 35, 603, 1439, 1684, 624, 523, 974, 957], 120: [89, 132, 509, 925, 1092, 28, 9, 259, 1376, 192, 788], 0: [0, 1, 637, 484, 496, 7, 491, 34, 280, 1609, 231], 283: [236, 235, 1050, 1297, 832, 1215, 386, 200, 403, 885, 181], 1128: [793, 1142, 1374, 789, 39, 64, 7, 335, 755, 470, 296], 165: [307, 308, 775, 167, 1572, 208, 852, 739, 1144, 618, 829], 147: [126, 151, 119, 60, 322, 32, 21, 368, 417, 12, 51], 912: [913, 1466, 1308, 745, 505, 441, 452, 141, 1088, 1648, 1397], 904: [749, 748, 911, 647, 1080, 431, 810, 70, 677, 435, 593], 77: [82, 25, 54, 1335, 1138, 817, 129, 1240, 424, 229, 894], 393: [347, 398, 171, 31, 248, 59, 1353, 1385, 237, 166, 250], 274: [222, 179, 1206, 1001, 445, 214, 418, 180, 625, 74, 1279], 1027: [1406, 226, 451, 614, 596, 1010, 243, 571, 755, 640, 51], 429: [406, 313, 1582, 813, 594, 799, 301, 1072, 1457, 724, 849], 1345: [1247, 937, 1360, 620, 1161, 870, 132, 1149, 791, 961, 840], 10: [9, 28, 81, 259, 132, 192, 96, 89, 655, 638, 487], 488: [425, 374, 1230, 1095, 1253, 1464, 1077, 1390, 407, 175, 218], 179: [135, 144, 723, 892, 15, 14, 360, 252, 363, 197, 621], 109: [76, 65, 42, 390, 1301, 407, 29, 773, 72, 123, 946], 34: [93, 188, 742, 711, 138, 337, 77, 91, 718, 80, 35], 1147: [1083, 1154, 1608, 95, 659, 302, 524, 1683, 88, 713, 271], 1214: [1332, 971, 378, 385, 1458, 1610, 687, 1407, 629, 293, 780], 160: [192, 259, 736, 1387, 9, 28, 89, 1555, 509, 925, 132], 193: [235, 236, 1215, 1297, 832, 1050, 885, 403, 599, 200, 386], 645: [615, 205, 178, 1287, 492, 518, 241, 459, 645, 770, 1583], 1434: [916, 1395, 1558, 708, 1505, 1155, 1681, 1698, 486, 1147, 411], 292: [244, 519, 508, 772, 1165, 1659, 940, 1404, 139, 1550, 576], 650: [725, 1040, 922, 663, 227, 288, 321, 1206, 336, 1001, 1464], 278: [195, 334, 614, 451, 1106, 1285, 174, 641, 506, 755, 1187], 641: [927, 1492, 1651, 1129, 1593, 337, 1664, 718, 1055, 771, 341], 145: [74, 625, 713, 364, 781, 2, 1030, 628, 51, 437, 33], 690: [719, 872, 920, 294, 1490, 1675, 1497, 183, 1402, 396, 1555], 199: [233, 210, 109, 688, 38, 84, 357, 715, 411, 1196, 1578], 967: [1568, 1524, 1149, 298, 881, 818, 1192, 259, 462, 870, 261], 476: [358, 285, 564, 381, 506, 453, 91, 138, 330, 275, 203], 8: [8, 3, 398, 546, 523, 894, 569, 198, 1076, 892, 837], 246: [476, 389, 1193, 1007, 1220, 851, 635, 176, 266, 99, 710], 449: [387, 738, 479, 869, 240, 239, 503, 124, 662, 1070, 1093], 348: [375, 430, 1187, 1136, 1474, 1181, 337, 1391, 755, 300, 1082], 521: [526, 515, 184, 107, 674, 338, 513, 633, 794, 157, 1191], 1302: [1209, 1176, 324, 919, 1164, 747, 889, 786, 485, 834, 619], 240: [150, 129, 691, 1156, 739, 1144, 1138, 586, 854, 54, 704], 74: [243, 226, 718, 1216, 337, 1136, 785, 362, 777, 232, 814], 1644: [1632, 924, 743, 1139, 1028, 1424, 1195, 1086, 734, 1490, 620], 68: [116, 55, 185, 431, 603, 270, 1330, 287, 674, 134, 578], 178: [270, 185, 116, 55, 70, 1330, 35, 312, 287, 134, 469], 223: [158, 111, 895, 1422, 356, 920, 317, 30, 619, 599, 903], 453: [513, 674, 289, 148, 107, 1246, 929, 184, 515, 325, 526], 281: [287, 134, 1330, 35, 70, 593, 312, 603, 116, 270, 1112], 115: [360, 363, 231, 892, 723, 280, 144, 135, 1286, 632, 321], 4: [1, 0, 637, 484, 7, 34, 280, 231, 1609, 1423, 1254], 909: [753, 1172, 1122, 455, 777, 379, 157, 396, 635, 527, 851], 36: [57, 58, 206, 155, 330, 275, 1688, 190, 311, 217, 923], 261: [208, 167, 1521, 861, 801, 1488, 183, 776, 1355, 307, 777], 736: [830, 815, 209, 239, 375, 285, 881, 167, 662, 1651, 249], 919: [828, 791, 1642, 1255, 57, 620, 937, 1121, 1046, 1247, 903], 486: [500, 273, 465, 948, 283, 903, 298, 1566, 1481, 928, 148], 306: [384, 295, 275, 330, 311, 190, 326, 546, 503, 431, 481], 132: [255, 362, 701, 862, 438, 1150, 300, 234, 893, 298, 670], 315: [245, 498, 139, 136, 621, 1163, 626, 125, 278, 885, 599], 38: [31, 46, 591, 340, 398, 347, 376, 1456, 744, 532, 627], 1560: [1439, 1688, 721, 874, 611, 546, 1635, 523, 58, 57, 923], 365: [153, 102, 643, 580, 199, 1402, 321, 25, 609, 490, 1006], 639: [759, 478, 264, 507, 1404, 1541, 1307, 1361, 717, 1453, 722], 64: [65, 76, 29, 42, 1301, 773, 390, 946, 416, 825, 407], 234: [391, 600, 127, 180, 1510, 691, 1542, 366, 1572, 1383, 221], 497: [446, 529, 1504, 1403, 1075, 1266, 417, 1441, 48, 841, 290], 75: [55, 116, 185, 603, 270, 431, 283, 674, 1330, 624, 134], 701: [1061, 1272, 1696, 1043, 1023, 1666, 974, 957, 1144, 1411, 739], 1479: [1055, 1062, 193, 104, 27, 63, 91, 1496, 927, 1492, 609], 925: [1331, 902, 399, 1179, 1495, 468, 1028, 960, 854, 984, 1219], 820: [888, 568, 583, 1667, 714, 1640, 1064, 1311, 886, 990, 868], 323: [149, 108, 207, 282, 943, 479, 503, 230, 145, 101, 967], 101: [102, 153, 643, 14, 15, 135, 580, 816, 199, 1402, 121], 937: [1131, 588, 805, 552, 1314, 1048, 697, 1437, 1458, 1536, 309], 441: [407, 390, 1329, 1236, 368, 54, 76, 425, 1293, 374, 37], 355: [385, 378, 760, 675, 1625, 1516, 1418, 1030, 781, 1698, 1293], 366: [368, 421, 51, 126, 32, 417, 151, 728, 322, 563, 1076], 661: [691, 1156, 129, 150, 1001, 391, 1330, 1259, 1206, 1183, 315], 1004: [986, 1037, 586, 25, 1102, 1188, 982, 1421, 82, 903, 1410], 280: [264, 507, 917, 1404, 759, 842, 1361, 868, 433, 294, 669], 97: [78, 75, 1177, 1190, 527, 379, 652, 396, 426, 539, 689], 404: [297, 332, 293, 460, 27, 52, 165, 350, 294, 696, 183], 65: [43, 44, 1150, 140, 145, 1263, 779, 906, 814, 234, 1575], 106: [280, 231, 360, 363, 7, 34, 1286, 951, 1, 491, 496], 616: [642, 757, 950, 979, 1120, 713, 1545, 364, 1034, 1511, 808], 1056: [1016, 1059, 1238, 1616, 1415, 1213, 956, 983, 1212, 446, 1129], 35: [17, 16, 85, 26, 18, 23, 1421, 421, 1188, 1260, 368], 346: [334, 195, 614, 451, 506, 453, 174, 1474, 874, 1187, 1106], 373: [634, 689, 304, 333, 456, 635, 461, 558, 710, 851, 396], 226: [175, 218, 133, 152, 787, 682, 586, 1067, 580, 745, 1308], 154: [109, 38, 257, 233, 130, 210, 84, 357, 688, 472, 715]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxYydeurkXd_",
        "outputId": "93899176-fb6a-44b2-db0d-6fa166b7350c"
      },
      "source": [
        "print(ground_truth)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{196: 188, 1359: 1461, 191: 154, 689: 647, 402: 419, 144: 189, 96: 71, 47: 38, 1597: 1162, 114: 105, 52: 68, 382: 330, 159: 136, 668: 1098, 82: 59, 334: 319, 43: 39, 19: 51, 321: 210, 1: 16, 502: 535, 182: 165, 78: 92, 827: 1031, 76: 61, 95: 85, 507: 374, 343: 265, 21: 14, 662: 742, 197: 215, 24: 35, 392: 346, 856: 529, 275: 259, 975: 561, 461: 452, 213: 223, 673: 752, 508: 579, 712: 987, 1045: 915, 85: 94, 266: 220, 887: 1187, 33: 22, 80: 46, 112: 104, 13: 40, 608: 539, 826: 842, 137: 128, 961: 1030, 733: 797, 482: 477, 459: 365, 40: 36, 57: 24, 573: 606, 434: 388, 124: 125, 784: 807, 400: 449, 549: 431, 646: 616, 375: 363, 142: 209, 552: 499, 740: 936, 394: 470, 59: 69, 663: 809, 425: 341, 249: 262, 308: 327, 81: 88, 113: 86, 419: 511, 22: 21, 749: 951, 1410: 1357, 143: 122, 1394: 1377, 309: 312, 421: 393, 44: 45, 26: 13, 29: 28, 1177: 1105, 974: 786, 153: 79, 753: 880, 851: 562, 357: 299, 15: 2, 408: 585, 1241: 857, 1219: 1339, 1018: 745, 287: 183, 541: 324, 585: 646, 108: 180, 110: 101, 162: 144, 1075: 943, 1091: 1254, 262: 232, 1677: 1577, 158: 143, 151: 156, 356: 356, 111: 84, 299: 260, 349: 424, 41: 12, 437: 417, 1080: 1348, 1370: 1364, 1314: 1224, 20: 18, 462: 475, 104: 91, 150: 242, 981: 864, 458: 447, 353: 282, 1400: 1301, 289: 333, 11: 6, 499: 569, 6: 10, 570: 571, 559: 502, 756: 961, 1462: 1448, 609: 479, 540: 463, 231: 200, 1046: 703, 558: 460, 161: 124, 374: 162, 279: 432, 563: 559, 526: 386, 84: 50, 127: 106, 882: 630, 25: 37, 87: 56, 748: 947, 450: 505, 362: 305, 79: 87, 269: 176, 252: 120, 409: 766, 354: 484, 791: 990, 1415: 1642, 204: 170, 48: 32, 117: 112, 581: 252, 1337: 1238, 911: 909, 16: 25, 1125: 683, 782: 885, 53: 117, 1246: 1575, 55: 27, 83: 97, 1610: 1472, 1202: 923, 31: 29, 12: 53, 1523: 1516, 596: 736, 811: 1109, 1283: 1303, 69: 60, 190: 167, 72: 64, 180: 146, 17: 3, 342: 390, 420: 455, 463: 508, 14: 4, 243: 292, 58: 90, 172: 164, 528: 567, 993: 1602, 825: 838, 436: 1189, 557: 345, 632: 890, 177: 197, 597: 574, 200: 412, 884: 494, 1180: 1382, 51: 66, 285: 182, 120: 89, 0: 0, 283: 236, 1128: 793, 165: 307, 147: 126, 912: 913, 904: 749, 77: 82, 393: 347, 274: 222, 1027: 1406, 429: 406, 1345: 1247, 10: 9, 488: 425, 179: 135, 109: 76, 34: 93, 1147: 1083, 1214: 1332, 160: 192, 193: 235, 645: 615, 1434: 916, 292: 244, 650: 725, 278: 195, 641: 927, 145: 74, 690: 719, 199: 233, 967: 1568, 476: 358, 8: 8, 246: 476, 449: 387, 348: 375, 521: 526, 1302: 1209, 240: 150, 74: 243, 1644: 1632, 68: 116, 178: 270, 223: 158, 453: 513, 281: 287, 115: 360, 4: 1, 909: 753, 36: 57, 261: 208, 736: 830, 919: 828, 486: 500, 306: 384, 132: 255, 315: 245, 38: 31, 1560: 1439, 365: 153, 639: 759, 64: 65, 234: 391, 497: 446, 75: 55, 701: 1061, 1479: 1055, 925: 1331, 820: 888, 323: 149, 101: 102, 937: 1131, 441: 407, 355: 385, 366: 368, 661: 691, 1004: 986, 280: 264, 97: 78, 404: 297, 65: 43, 106: 280, 616: 642, 1056: 1016, 35: 17, 346: 334, 373: 634, 226: 175, 154: 109}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8AzAYAkUGpG"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ChTlRxHEBBX"
      },
      "source": [
        "G1_adj_matrix, G2_adj_matrix, emb1, emb2, ground_truth = build_adj_matrix_and_embeddings()"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSMyVossbuEn"
      },
      "source": [
        "# first_adj_matrix, second_adj_matrix = build_adj_maxtrix()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "first_embeddings_torch=torch.from_numpy(emb1).type(torch.FloatTensor).to(device)\n",
        "second_embeddings_torch=torch.from_numpy(emb2).type(torch.FloatTensor).to(device)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOgGxetobxMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d90ec0-494e-402e-fb23-4d43ca3de229"
      },
      "source": [
        "lr = 0.0001\n",
        "torch.manual_seed(500)\n",
        "net = Agent(G1_adj_matrix, G2_adj_matrix, emb1.shape[1], 16, 1, activation='Sigmoid')\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "net.to(device)\n",
        "net.train()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Agent(\n",
              "  (layer1): GCN_layer()\n",
              "  (layer2): GCN_layer()\n",
              "  (activation): Sigmoid()\n",
              "  (softmax): Softmax(dim=1)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GjOgiHCUHm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25bfdaca-bfe5-4bc1-d5a4-8d3a5a7da397"
      },
      "source": [
        "env = SequentialMatchingEnv()\n",
        "gamma = 0.99\n",
        "rewards_lst = [] \n",
        "episodes = 1\n",
        "lst_state = []\n",
        "results = []\n",
        "\n",
        "for ep in tqdm(range(1, episodes + 1)):\n",
        "  idx = env.reset()\n",
        "  memory = Memory()\n",
        "  reward_episode = 0\n",
        "  done = False\n",
        "  # define policy and action\n",
        "  while True:\n",
        "    # get environment state\n",
        "    # print('first_embeddings_torch - second_embeddings_torch: {} \\n {}'.format(first_embeddings_torch, second_embeddings_torch))\n",
        "    action = net.get_action(first_embeddings_torch, second_embeddings_torch, idx)\n",
        "    cur_idx, next_idx, obs, reward, done, info = env.step(action, ep)\n",
        "    if done:\n",
        "      break\n",
        "    # add reward \n",
        "    reward_episode += reward\n",
        "\n",
        "    # push to memory for training model\n",
        "    action_one_hot = torch.zeros(2)\n",
        "    action_one_hot[action] = 1\n",
        "    memory.push(next_idx, action_one_hot, reward)\n",
        "\n",
        "    # next state\n",
        "    idx = next_idx\n",
        "    lst_state.append((cur_idx, action, reward))\n",
        "    \n",
        "  loss = net.train_model(first_embeddings_torch, second_embeddings_torch, net, memory.sample(), optimizer)\n",
        "  results.append([ep, reward_episode, loss.cpu().detach().numpy()])\n",
        "  if reward_episode > 15:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break\n",
        "\n",
        "results = np.array(results)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [06:25<00:00, 385.21s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMV6OtdcTBgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "37f1bf94-5639-47d8-b6ad-171f31b29a14"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "results = pd.DataFrame(results, columns=['episode', 'reward', 'agent_loss'])\n",
        "sns.lineplot(data=results.reward, color=\"g\")\n",
        "ax2 = plt.twinx()\n",
        "sns.lineplot(data=results.agent_loss, color=\"b\", ax=ax2)\n",
        "plt.show()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq5_03j0jfrs",
        "outputId": "a777c9b7-a5ff-4a02-e998-b41377e7a455"
      },
      "source": [
        "print(lst_state)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[((196, 188), 0, -10), ((196, 93), 0, 0), ((196, 742), 0, 0), ((196, 711), 0, 0), ((196, 718), 0, 0), ((196, 337), 1, 0), ((161, 662), 1, 0), ((179, 892), 0, 0), ((645, 615), 0, -10), ((1677, 1392), 0, 0), ((321, 357), 1, 0), ((1479, 927), 1, 0), ((820, 1064), 0, 0), ((753, 346), 1, 0), ((19, 417), 1, 0), ((243, 584), 0, 0), ((334, 252), 1, 0), ((497, 1266), 1, 0), ((95, 1515), 1, 0), ((609, 209), 1, 0), ((462, 1486), 1, 0), ((15, 1454), 0, 0), ((1394, 619), 0, 0), ((748, 1302), 0, 0), ((421, 228), 1, 0), ((904, 1080), 0, 0), ((1018, 1466), 0, 0), ((269, 304), 0, 0), ((608, 459), 1, 0), ((1523, 1625), 0, 0), ((143, 806), 0, 0), ((701, 1411), 0, 0), ((650, 663), 1, 0), ((541, 324), 0, -10), ((72, 67), 0, 0), ((419, 502), 1, 0), ((22, 12), 0, 0), ((756, 1247), 0, 0), ((1434, 1558), 0, 0), ((887, 375), 1, 0), ((1610, 1042), 1, 0), ((374, 531), 0, 0), ((1462, 1448), 1, 1), ((476, 506), 0, 0), ((825, 1045), 0, 0), ((645, 770), 1, 0), ((261, 861), 0, 0), ((120, 9), 0, 0), ((262, 580), 1, 0), ((967, 1568), 0, -10), ((269, 99), 0, 0), ((437, 119), 1, 0), ((38, 31), 0, -10), ((76, 281), 1, 0), ((97, 689), 1, 0), ((668, 1603), 0, 0), ((285, 35), 1, 0), ((197, 449), 0, 0), ((508, 826), 0, 0), ((275, 574), 0, 0), ((458, 354), 0, 0), ((486, 298), 1, 0), ((240, 54), 1, 0), ((80, 591), 1, 0), ((1677, 1064), 1, 0), ((919, 1121), 0, 0), ((1314, 1327), 0, 0), ((354, 34), 0, 0), ((14, 1175), 1, 0), ((749, 363), 1, 0), ((34, 711), 0, 0), ((362, 34), 0, 0), ((1302, 889), 0, 0), ((274, 214), 1, 0), ((11, 1602), 1, 0), ((191, 367), 0, 0), ((283, 403), 1, 0), ((925, 960), 0, 0), ((84, 683), 0, 0), ((1410, 1271), 0, 0), ((1560, 1688), 0, 0), ((223, 30), 0, 0), ((193, 386), 0, 0), ((434, 355), 1, 0), ((72, 557), 1, 0), ((308, 1442), 0, 0), ((1302, 1164), 0, 0), ((226, 1067), 1, 0), ((353, 68), 1, 0), ((646, 1534), 1, 0), ((193, 200), 1, 0), ((508, 672), 1, 0), ((458, 1501), 0, 0), ((59, 861), 1, 0), ((354, 496), 0, 0), ((1202, 224), 1, 0), ((10, 638), 0, 0), ((178, 312), 0, 0), ((967, 870), 0, 0), ((365, 1402), 0, 0), ((981, 966), 1, 0), ((1027, 571), 1, 0), ((53, 1273), 0, 0), ((1434, 916), 0, -10), ((1241, 754), 0, 0), ((673, 752), 0, -10), ((108, 1075), 1, 0), ((75, 603), 1, 0), ((733, 1581), 1, 0), ((668, 1472), 1, 0), ((374, 1514), 0, 0), ((585, 794), 0, 0), ((213, 569), 0, 0), ((52, 215), 1, 0), ((1125, 230), 0, 0), ((41, 21), 0, 0), ((425, 585), 0, 0), ((362, 1333), 0, 0), ((1056, 956), 0, 0), ((549, 70), 0, 0), ((16, 766), 1, 0), ((180, 373), 1, 0), ((1125, 882), 0, 0), ((40, 106), 1, 0), ((6, 1278), 0, 0), ((791, 436), 0, 0), ((124, 621), 0, 0), ((213, 593), 0, 0), ((4, 7), 1, 0), ((6, 722), 1, 0), ((404, 183), 1, 0), ((29, 9), 1, 0), ((234, 1510), 1, 0), ((825, 679), 0, 0), ((1434, 708), 0, 0), ((450, 913), 0, 0), ((306, 503), 1, 0), ((1246, 556), 0, 0), ((182, 290), 0, 0), ((1359, 280), 1, 0), ((458, 113), 1, 0), ((408, 678), 1, 0), ((299, 201), 1, 0), ((159, 621), 1, 0), ((79, 1134), 0, 0), ((44, 649), 1, 0), ((269, 333), 0, 0), ((127, 36), 1, 0), ((884, 794), 1, 0), ((1241, 351), 0, 0), ((266, 48), 0, 0), ((17, 894), 0, 0), ((15, 74), 1, 0), ((38, 532), 0, 0), ((402, 454), 0, 0), ((1080, 1323), 0, 0), ((429, 799), 0, 0), ((13, 32), 1, 0), ((1597, 123), 0, 0), ((1410, 1233), 1, 0), ((158, 1606), 1, 0), ((488, 1390), 0, 0), ((145, 1030), 0, 0), ((1177, 1105), 1, 1), ((151, 438), 1, 0), ((47, 257), 0, 0), ((0, 1609), 0, 0), ((144, 447), 1, 0), ((409, 506), 1, 0), ((249, 251), 0, 0), ((101, 1402), 1, 0), ((1523, 385), 1, 0), ((308, 516), 1, 0), ((1056, 1212), 1, 0), ((1180, 709), 0, 0), ((182, 165), 0, -10), ((1219, 947), 1, 0), ((394, 470), 0, -10), ((820, 868), 0, 0), ((912, 141), 0, 0), ((382, 155), 0, 0), ((1302, 1209), 1, 1), ((993, 1271), 0, 0), ((558, 894), 0, 0), ((83, 90), 1, 0), ((1246, 43), 0, 0), ((17, 546), 0, 0), ((552, 1298), 1, 0), ((20, 467), 0, 0), ((461, 745), 0, 0), ((104, 783), 1, 0), ((912, 1648), 0, 0), ((453, 325), 0, 0), ((111, 84), 0, -10), ((1091, 1), 1, 0), ((117, 445), 1, 0), ((597, 574), 1, 1), ((1004, 1188), 1, 0), ((639, 717), 1, 0), ((178, 1330), 0, 0), ((1345, 961), 0, 0), ((154, 472), 0, 0), ((507, 1230), 0, 0), ((26, 30), 1, 0), ((392, 1015), 0, 0), ((925, 1331), 1, 1), ((154, 257), 1, 0), ((392, 524), 0, 0), ((64, 825), 1, 0), ((33, 302), 0, 0), ((269, 389), 1, 0), ((967, 818), 0, 0), ((573, 2), 1, 0), ((784, 807), 1, 1), ((1128, 470), 1, 0), ((1644, 620), 0, 0), ((87, 1475), 0, 0), ((394, 789), 1, 0), ((1560, 611), 0, 0), ((87, 736), 1, 0), ((1394, 1635), 0, 0), ((1125, 50), 1, 0), ((78, 230), 1, 0), ((441, 407), 0, -10), ((22, 436), 1, 0), ((114, 992), 0, 0), ((425, 128), 0, 0), ((165, 1144), 0, 0), ((882, 1073), 1, 0), ((740, 374), 1, 0), ((904, 647), 1, 0), ((690, 1490), 1, 0), ((673, 701), 0, 0), ((856, 128), 1, 0), ((58, 123), 1, 0), ((1147, 1083), 0, -10), ((791, 170), 0, 0), ((791, 1483), 1, 0), ((482, 638), 1, 0), ((748, 755), 0, 0), ((1337, 1213), 0, 0), ((641, 771), 1, 0), ((162, 723), 1, 0), ((120, 89), 1, 1), ((449, 869), 0, 0), ((112, 1454), 0, 0), ((24, 287), 0, 0), ((820, 888), 0, -10), ((1241, 929), 0, 0), ((1046, 1217), 0, 0), ((143, 122), 1, 1), ((558, 5), 1, 0), ((172, 141), 1, 0), ((110, 1689), 0, 0), ((563, 250), 1, 0), ((179, 197), 1, 0), ((348, 1391), 1, 0), ((365, 199), 0, 0), ((51, 45), 1, 0), ((112, 466), 1, 0), ((309, 1112), 0, 0), ((20, 315), 1, 0), ((178, 55), 0, 0), ((279, 432), 1, 1), ((261, 1521), 1, 0), ((1370, 1276), 1, 0), ((827, 176), 0, 0), ((661, 1001), 1, 0), ((616, 1545), 1, 0), ((246, 1220), 0, 0), ((41, 902), 0, 0), ((342, 352), 0, 0), ((69, 322), 1, 0), ((177, 1265), 0, 0), ((43, 934), 0, 0), ((47, 109), 0, 0), ((570, 416), 0, 0), ((1214, 1332), 1, 1), ((1345, 132), 0, 0), ((528, 1173), 1, 0), ((449, 124), 0, 0), ((1415, 828), 0, 0), ((1147, 1608), 1, 0), ((1241, 886), 1, 0), ((109, 42), 1, 0), ((912, 913), 1, 1), ((323, 943), 1, 0), ((223, 619), 1, 0), ((362, 254), 1, 0), ((287, 776), 1, 0), ((65, 140), 0, 0), ((581, 491), 0, 0), ((429, 313), 1, 0), ((1434, 1155), 1, 0), ((499, 232), 0, 0), ((190, 801), 0, 0), ((1080, 890), 0, 0), ((441, 1293), 0, 0), ((549, 431), 0, -10), ((476, 453), 0, 0), ((449, 1093), 1, 0), ((541, 1164), 1, 0), ((1394, 1436), 1, 0), ((911, 1554), 0, 0), ((488, 1230), 1, 0), ((43, 98), 1, 0), ((961, 1194), 0, 0), ((632, 1323), 1, 0), ((1246, 1256), 0, 0), ((1246, 1597), 0, 0), ((34, 138), 1, 0), ((1415, 754), 0, 0), ((1597, 267), 0, 0), ((382, 330), 1, 1), ((937, 1131), 0, -10), ((147, 12), 1, 0), ((114, 105), 0, -10), ((911, 1362), 0, 0), ((266, 572), 1, 0), ((79, 693), 0, 0), ((756, 732), 1, 0), ((425, 414), 1, 0), ((177, 1206), 1, 0), ((55, 27), 0, -10), ((1337, 1059), 1, 0), ((827, 833), 1, 0), ((124, 245), 1, 0), ((911, 1628), 0, 0), ((53, 746), 1, 0), ((204, 189), 0, 0), ((570, 1463), 1, 0), ((701, 1696), 1, 0), ((851, 439), 0, 0), ((937, 1048), 0, 0), ((1080, 1387), 1, 0), ((507, 1390), 1, 0), ((25, 37), 0, -10), ((461, 164), 1, 0), ((289, 176), 0, 0), ((402, 1115), 0, 0), ((366, 728), 0, 0), ((57, 204), 0, 0), ((77, 817), 0, 0), ((111, 809), 1, 0), ((993, 413), 1, 0), ((150, 1537), 0, 0), ((975, 229), 1, 0), ((919, 1255), 1, 0), ((851, 1663), 1, 0), ((10, 487), 0, 0), ((1045, 862), 0, 0), ((1018, 339), 1, 0), ((673, 670), 0, 0), ((82, 182), 0, 0), ((712, 705), 0, 0), ((81, 1085), 0, 0), ((402, 1329), 1, 0), ((820, 714), 0, 0), ((585, 816), 1, 0), ((420, 935), 0, 0), ((17, 892), 1, 0), ((540, 555), 0, 0), ((8, 1076), 1, 0), ((909, 635), 0, 0), ((0, 491), 0, 0), ((526, 832), 0, 0), ((521, 338), 0, 0), ((38, 46), 1, 0), ((1400, 1597), 0, 0), ((909, 455), 1, 0), ((1345, 870), 0, 0), ((278, 614), 1, 0), ((65, 906), 0, 0), ((10, 192), 1, 0), ((521, 633), 1, 0), ((499, 674), 0, 0), ((24, 1112), 1, 0), ((142, 282), 1, 0), ((420, 216), 0, 0), ((204, 170), 0, -10), ((114, 296), 0, 0), ((81, 1488), 0, 0), ((540, 780), 0, 0), ((57, 170), 1, 0), ((1180, 191), 0, 0), ((114, 598), 1, 0), ((115, 1286), 1, 0), ((559, 345), 0, 0), ((160, 28), 0, 0), ((0, 0), 0, -10), ((213, 182), 0, 0), ((246, 1007), 1, 0), ((65, 145), 1, 0), ((85, 94), 0, -10), ((463, 1404), 1, 0), ((204, 543), 0, 0), ((150, 242), 0, -10), ((1400, 1301), 1, 1), ((549, 578), 0, 0), ((1283, 534), 0, 0), ((191, 1619), 1, 0), ((400, 1032), 0, 0), ((77, 424), 1, 0), ((1345, 1149), 0, 0), ((1345, 1161), 0, 0), ((82, 1224), 0, 0), ((1560, 1439), 1, 1), ((596, 698), 0, 0), ((243, 351), 0, 0), ((526, 1215), 0, 0), ((392, 1559), 0, 0), ((826, 826), 0, 0), ((160, 1555), 1, 0), ((420, 527), 1, 0), ((559, 1118), 0, 0), ((441, 390), 1, 0), ((292, 1659), 1, 0), ((25, 1691), 1, 0), ((85, 165), 0, 0), ((182, 294), 0, 0), ((1246, 1575), 0, -10), ((31, 416), 1, 0), ((937, 697), 0, 0), ((309, 270), 0, 0), ((393, 347), 0, -10), ((349, 82), 0, 0), ((820, 1667), 0, 0), ((249, 1543), 1, 0), ((453, 148), 0, 0), ((178, 70), 0, 0), ((549, 538), 1, 0), ((275, 259), 0, -10), ((476, 91), 0, 0), ((1180, 492), 0, 0), ((961, 705), 0, 0), ((356, 267), 1, 0), ((1045, 915), 1, 1), ((150, 418), 1, 0), ((55, 325), 0, 0), ((393, 398), 1, 0), ((365, 102), 0, 0), ((343, 852), 0, 0), ((289, 456), 0, 0), ((1644, 1028), 0, 0), ((526, 181), 0, 0), ((182, 895), 1, 0), ((450, 893), 0, 0), ((1046, 247), 0, 0), ((1644, 1632), 0, -10), ((1415, 1063), 0, 0), ((153, 79), 1, 1), ((526, 235), 1, 0), ((974, 1688), 1, 0), ((911, 989), 0, 0), ((132, 1150), 0, 0), ((453, 1246), 1, 0), ((1314, 1181), 1, 0), ((96, 212), 0, 0), ((499, 871), 0, 0), ((826, 509), 0, 0), ((343, 265), 1, 1), ((499, 428), 1, 0), ((47, 688), 0, 0), ((1046, 703), 1, 1), ((354, 484), 1, 1), ((596, 890), 1, 0), ((1283, 694), 0, 0), ((231, 1050), 1, 0), ((84, 479), 0, 0), ((48, 712), 0, 0), ((165, 307), 0, -10), ((85, 350), 1, 0), ((41, 1118), 1, 0), ((961, 1030), 1, 1), ((48, 302), 1, 0), ((0, 637), 1, 0), ((82, 624), 1, 0), ((33, 553), 0, 0), ((748, 1648), 0, 0), ((275, 973), 1, 0), ((540, 670), 1, 0), ((178, 185), 0, 0), ((712, 1430), 0, 0), ((1180, 1014), 1, 0), ((96, 71), 1, 1), ((712, 543), 0, 0), ((81, 88), 1, 1), ((190, 1488), 0, 0), ((55, 1062), 1, 0), ((199, 233), 0, -10), ((137, 251), 0, 0), ((826, 788), 0, 0), ((165, 208), 0, 0), ((366, 126), 1, 0), ((1283, 1303), 1, 1), ((1644, 743), 0, 0), ((1415, 1669), 1, 0), ((782, 832), 1, 0), ((204, 958), 0, 0), ((450, 505), 0, -10), ((309, 593), 0, 0), ((673, 555), 0, 0), ((937, 588), 0, 0), ((811, 1668), 0, 0), ((289, 630), 1, 0), ((559, 248), 1, 0), ((1597, 1087), 1, 0), ((1644, 1139), 1, 0), ((243, 66), 0, 0), ((476, 564), 1, 0), ((137, 773), 0, 0), ((712, 987), 0, -10), ((748, 1340), 0, 0), ((165, 829), 0, 0), ((342, 405), 1, 0), ((349, 1085), 1, 0), ((47, 130), 1, 0), ((165, 775), 0, 0), ((712, 1372), 0, 0), ((243, 1376), 0, 0), ((213, 523), 1, 0), ((132, 893), 0, 0), ((165, 1572), 1, 0), ((33, 27), 1, 0), ((712, 189), 1, 0), ((400, 1498), 0, 0), ((199, 715), 0, 0), ((826, 685), 1, 0), ((967, 1192), 1, 0), ((911, 652), 1, 0), ((937, 309), 0, 0), ((748, 1082), 0, 0), ((190, 307), 0, 0), ((937, 1437), 1, 0), ((199, 688), 1, 0), ((190, 208), 0, 0), ((811, 955), 1, 0), ((243, 576), 1, 0), ((178, 116), 0, 0), ((132, 862), 0, 0), ((137, 1480), 0, 0), ((748, 1650), 0, 0), ((673, 755), 0, 0), ((84, 325), 0, 0), ((450, 1413), 0, 0), ((673, 862), 0, 0), ((365, 490), 0, 0), ((204, 660), 0, 0), ((309, 469), 0, 0), ((581, 336), 0, 0), ((392, 309), 0, 0), ((132, 300), 1, 0), ((673, 1474), 0, 0), ((450, 745), 1, 0), ((365, 643), 1, 0), ((178, 270), 1, 1), ((820, 990), 0, 0), ((392, 465), 0, 0), ((137, 1210), 0, 0), ((204, 865), 1, 0), ((748, 186), 1, 0), ((1345, 1360), 1, 0), ((581, 1265), 0, 0), ((392, 1131), 1, 0), ((673, 1187), 0, 0), ((673, 1082), 1, 0), ((84, 148), 0, 0), ((400, 92), 0, 0), ((190, 1355), 0, 0), ((820, 1640), 0, 0), ((309, 1330), 0, 0), ((110, 92), 0, 0), ((309, 185), 0, 0), ((820, 568), 0, 0), ((820, 1311), 1, 0), ((581, 135), 0, 0), ((309, 134), 1, 0), ((110, 207), 1, 0), ((21, 102), 0, 0), ((21, 135), 0, 0), ((190, 167), 0, -10)]\n"
          ]
        }
      ]
    }
  ]
}